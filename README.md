# 摘要

电子显微镜下双目图像的物体断裂表面形状的三维重建是微观力学和断裂力学的研究应用。 三维形状重建是通过分析双目图像上匹配对应的点来实现的。 "interesting points" 可以有效的减少匹配失败的问题，同时也减少了大量的CPU计算时间。 "interesting points" 是在像素级别上搜索到的双目图像中重要的特征点，然后仅匹配这些"interesting points"。本文提供了两个搜索 "interesting points"算法。第一个算法是选择局部熵\(local entropy\)最大值的像素点。  第二个方法首先对图像使用基于拉普拉斯算子的高斯滤波处理，然后选择拥有局部最大灰度值的像素点。 同时，为了减少影子的影响，在高斯滤波之前，需要对图像强度进行一次对数计算。 在双目图像中匹配对应的点，是基于互相关\(cross correlation\)的理论。 为了减少匹配失败，本文使用了由粗到细的匹配算法\(a coarse and fine matching algorithm\)。 最后，物体表面的高度是基于双目三角测试理论进行计算的。

关键词：  形状重建\(shape reconstruction\) ， 互相关\(cross correlation\)， 图像处理\(image processing\)，实验力学\(experimental mechanics\)

# 1.介绍

在微观力学中，物体材质的研究是微型的或更小的规模。在这种角度下，材料的表面是非常粗糙的。 微观力学从3个维度来研究物体的材质，并进行测量：外形，形变 和 应力。这三个维度的测量变的越来越重要，这是因为所有的理论都需要基于一个实验结果，并且在实验中验证。而在所有的这些测量技术中， 外形的测定需要使用计算机视觉和图像处理,这两个都不是非常成熟的领域。

在计算机视觉领域的形状重建领域，有大量的令人印象深刻的研究结论\[引用中的2，3，7，8，9，10，11，12\]， 但是大部分都集中在宏观项目的三维重建，然而物体特征和表面在微米级别有一些明显的不同。有好多种方法可以对物体进行三维重建， 比如双目图像，纹理，明暗阴影。 同纹理和明暗阴影相比，双目图像的物体外形重建需要相对较少的假设。同时，在电子显微镜下可以通过改变样本的角度来获得双目图像。因此，在大多数例子中，双目图像和其他方法相比，拥有有更高的精准度。

电子显微镜\(SEM\)下， 在双目图像中匹配对应点的研究已经在应用了\(Russ\[1\], Lee\[4\], Thompson\[5\]  , and Moravec\[6\]\)， 它是基于互相关\(cross correlation\)理论。为了减少运算的时间消耗，本文开发出了搜索"interesting points"的算法，这个算法仅仅在已经选中的像素点中匹配。匹配窗口的大小\(matching-window size\)的效果也在一些计算机视觉的研究者中进行讨论，因此在未来还需要开发出更鲁棒和更快的算法。

本篇论文提供了一个物体形状的三维重建算法，这个算法是根据电子显微镜下拿到的双目图像，针对破裂物体的表面的进行三维重建。第二部分，开发并讨论了两个搜索"interesting points"的算法，并对基于互相关\(cross correlation\) 理论的"interesting points"匹配算法进行了详细的讨论。第三部分，给出了几个实验结果， 包括验证了真实的断裂的表面重建的效果。第四部分是讨论和结论。

# 2.算法

本算法是从一对双目图像中提取信息，并计算出表面的立体图。这个任务困难的地方是从两幅图中找出对应的位置，这是因为目标匹配失败问题\(false-target problem\)。 我们知道，在人类的视觉中，由于视觉上的错觉，目标匹配失败\(false stereo-matching\)也是会发生的。因此，匹配问题是算法首先要解决的第一个问题。

### 2.1 搜索 interesting points

从两张图中匹配每一个点是计算密集型任务，而且有时甚至是行不通的，因为在搜索范围内有非常多相似的特征点，这些点可能比正确的点还要像。因此，搜索"interesting points"是我们算法中第一个重要的事情，之后的匹配则仅仅针对"interesting points"。

"interesting points" 是一对图片中重要特征的像素位置。通常他们是倾斜的并且不连续的。或者和其他像素点相比，有比较大的局部差异或者明显的亮度的偏差。  

简单的匹配"interesting points" 和 匹配像素中的每一个点相比，速度并不会快，但它会更鲁棒。 从"interesting points" 的定义可以看出，"interesting poinsts"在局部的亮度中一个有很大的差异，在互相关\(cross correlation\)中是一个很大的值，和周围的点相比会有很大的差异。 

##### 1\) 搜索"interesting points"的两个算法：



**a. 局部熵算法**

第一个是将局部熵的最大值作为"interesting points"。  局部熵的计算是基于周围NxN的邻居每个点的亮度值，如下图：

![](evernotecid://FE6EF2C5-C843-42E9-A0E4-9CD833151B9C/ENResource/p16877)

  
其中： g\_i是第i个点的灰度值。 n\(g\_i\)是一个比率， 是在匹配区域中，和g\_i点灰度值相同的所有像素点的数量/总的像素点的数量。 

**b.高斯滤波器算法**

另一个搜索 "interesting points"的方法是使用拉普拉斯算子的高斯滤波器。 为了减少影子的影响，需要首先对图像强度进行一次对数计算，然后使用拉普拉斯算子的高斯滤波器进行卷积计算 。  在我们的程序中，高斯滤波的标准差是1.5像素。

![](evernotecid://FE6EF2C5-C843-42E9-A0E4-9CD833151B9C/ENResource/p16873)

  
其中 

![](evernotecid://FE6EF2C5-C843-42E9-A0E4-9CD833151B9C/ENResource/p16886)

是拉普拉斯算子的高斯滤波器的核。 "interesting points"是拉普拉斯的高斯滤波后，局部值最大的那个点。

"interesting points"的数量依赖于图像的特征，而经常的情况是"interesting points"的数量非常的小。 比如一张512x512的图片可能只有几千个 "interesting points"

### 2.2 匹配对应点

匹配对应点的算法是基于互相关\(cross correlation\)的理论。匹配点是找出最相似的那个点。我们会将双目两个图像中的进行图片分片，划分为若干个片断，然后对图片片断逐个处理。我们定义f\(i,j\)是第一张图像的图片片断中 \(i,j\) 点的灰度值，g\(i,j\) 是第二张图像对应的图片片断中对应的\(i,j\)点的灰度值。 然后归一化互相关\(cross correlation\) 函数就可以计算出每一个相关点\(m,n\)的值并用于测试。

  


![](evernotecid://FE6EF2C5-C843-42E9-A0E4-9CD833151B9C/ENResource/p16880)

  


  


考虑到不同光照强度下，一张图片可能会认为是两张不同的图片的情况，需要对f\(i,j\) 和 g\(i,j\) 进行归一化处理，归一化处理的方法是  减去两个灰度值的均值 f\_m 和g\_m， 如下面表达式的那样：

delta\_f是第一张图像的图片片断中灰度值的标准差。delta\_g\(m,n\)是第二个图中，对应位置的图片片断中灰度值的标准差。 我们用法如下：

![](evernotecid://FE6EF2C5-C843-42E9-A0E4-9CD833151B9C/ENResource/p16881)



需要的是，匹配时仅仅处理之前定义的"interesting points"，而并不是所有的像素点。  另一方面，我们需要知道，并不是所有能够在两张图中对应上的点都是"interesting"， 所以，在匹配时，除了互相关的最大值之外，还应该有一些约束。在算法中，先定义了一个粗糙的阈值作为筛选出候选者的条件，然后在这个候选者周围的区域执行进行一个精细的搜索。 这个搜索的结果必然远大于 阈值,进而被认为是正确的的点。

为了减少搜索域，要搜索窗口的位置是由周围的点的视差来决定的,这依赖于表面连续性的假设，而且大多数情况是符合这个假设的。 匹配第一个点时，是没有有效的视差的，所以，程序会从用户那里接收一个估计值，并且搜索域也比其他的点要大。

### 2.3 表面高度的估计

### 

![](evernotecid://FE6EF2C5-C843-42E9-A0E4-9CD833151B9C/ENResource/p16875)



图1 显示了计算表面高度的原理

考虑两个匹配点 A 和B。  左图的A 和 右图的A'匹配 ，  左图的B 和右图的B' 匹配。 左图中，点A和B 的距离用水平线d1表示。 而右图，则经过了一个已知的角度 delta 的偏移，于是点A'  和 B' 会有一个新的距离 d2.  原图中两个点之间的重直起伏\(vertical relief\)用下面的表达式表达：

![](evernotecid://FE6EF2C5-C843-42E9-A0E4-9CD833151B9C/ENResource/p16871)

